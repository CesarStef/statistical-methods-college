---
title: "Project: College Dataset"
author: "Stefano Cattonar, Ines El Gataa, Andrija Nicić, Angelica Rota"
date: "2025-01-90"
output:
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(corrplot)
library(caret)
```

# Introduction

This project aims to analyze the `College` dataset from *An Introduction to Statistical Learning* to explore the factors that influence the number of applications received by colleges, represented by the response variable `Apps`. The dataset provides comprehensive information on various attributes of U.S. colleges, such as tuition costs, room and board expenses, financial aid, and student demographics. By examining these factors, we seek to better understand the trends and drivers behind college application rates.

# Preprocessing

## Data Examination

```{r}
College = read.csv("https://www.statlearning.com/s/College.csv", header = TRUE)
help(head)
head(College)
```

`X`: name of the college

`Private`: A factor with levels No and Yes indicating private or public university

`Accept`: Number of applications accepted

`Enroll`: Number of new students enrolled

`Top10perc`: Percentage new students from top 10% of high school class

`Top25perc`: Percentage new students from top 25% of high school class

`F.Undergrad`: Number of full time undergraduates

`P.Undergrad`: Number of part time undergraduates

`Outstate`: Out-of-state tuition

`Room.Board`: Room and board costs

`Books`: Estimated book costs

`Personal`: Estimated personal spending

`PhD`: Percentage of faculty with Ph.D.’s

`Terminal`: Percentage of faculty with terminal degree

`S.F.Ratio`: Student/faculty ratio

`perc.alumni`: Percentage alumni who donate

`Expend`: Instructional expenditure per student

`Grad.Rate`: Graduation rate

**Response variable**

`Apps`: Number of applications received


## Data Exploration 

```{r}
summary(College)
```


### Observations

1. We can note that the `X` column will yield no usable information.

```{r}
College <- select(College, -X)
```

2. We have to exclude also `Accept` and `Enroll` to prevent data leakage.

```{r}
College <- select(College, -c(Accept, Enroll))

summary(College)
```

3. All the variables does not have negative values.

4. The variables does not have NA, so we don't need to replace the missing values

5. We want to check if we have some duplicates

```{r}
idx <- which(duplicated(College))
idx
```

6. The variable `Private` is a categorical one so we convert its values.

```{r}
College$Private <- as.factor(College$Private)

College$Private <- as.numeric(College$Private) - 1  # "Yes" is 1, "No" is 0

College$Private[11:30]
```

7. We can note that `Private` is not balanced 

```{r}


index_dataset_public <- which(College$Private == 0)

index_dataset_private <- which(College$Private == 1)

private_proportion <- (length(index_dataset_private) / length(index_dataset_public))

table(College$Private)
private_proportion
```
We can see from `private_proportion` that "Yes" is 2.66 times more frequent than "No".

Now we can see the barplot

```{r}
barplot(table(College$Private), main="Private")
```

## Variable visualization


 Now we can visualize the variables and their distributions.

 Boxplot, excluded `Private` that is a categorical variable
```{r}

par(mfrow=c(1,3))
for (i in 1:length(College)) {
  if ( names(College[i]) != "Private"){
    boxplot(College[,i], main=names(College[i]))
  }
}
```

Histogram, excluded `Private` that is a categorical variable

```{r}
par(mfrow=c(1,3))
for (i in 1:length(College)) {
  if ( names(College[i]) != "Private"){
    hist(College[,i], main=names(College[i]))
  }
}
```

We can see that F.Undergrad and P.Undergrad are really skewed so we can take the logarithm.

```{r}
College$F.Undergrad <- log(College$F.Undergrad)
College$P.Undergrad <- log(College$P.Undergrad)

par(mfrow = c(2, 2))

boxplot(College$F.Undergrad, main="Full time undergraduates")
boxplot(College$P.Undergrad, main= "Part time undergraduates")

hist(College$F.Undergrad, main="Full time undergraduates")
hist(College$P.Undergrad, main= "Part time undergraduates")
```


## Correlation matrix

We plot the correlation matrix to examine the relationships between the predictors and `Apps`

```{r}
correlation_matrix <- cor(College)

corrplot(correlation_matrix, method="color", tl.cex = 0.8)
```

As you can see from the corralation matrix, there are some variables that are correlated with each other. We can see that the variables `Terminal` and `PhD` are really directly heavily correlated. This fenomena is caused by the fact that `PhD` containes a sub group of `Terminal`. So we can exclude `PhD`.

`Top10perc` and `Top25perc` are the same so for the same reason we can exclude `Top10perc`.

Now we can visualize some of the correlated variables

```{r}
boxplot(College$F.Undergrad ~ College$Private)
boxplot(College$P.Undergrad ~ College$Private)
boxplot(log(College$Apps) ~ College$Private)
```

Now let's normalize the data so that we have all the data with comparable scales.

```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

college_norm <- College
college_norm[ , 2:ncol(College)] <- lapply(College[ , 2:ncol(College)], normalize)

summary(college_norm)
```

# FROM HERE MERGEEEEEE!!!!!

Now we start to clean the dataset as told before:

```{r}

#Copy the dataset
college_cleared <- college_norm

#Remove the target variable and the variables that are not significant
college_cleared$Accept <- NULL
college_cleared$Enroll <- NULL
college_cleared$X <- NULL
college_cleared$Apps <- NULL
college_cleared$Top10perc <- NULL
college_cleared$PhD <- NULL

```

Now we must split the dataset in train set and test set.
We will use 80% of the dataset for the training and 20% for the test.

```{r}
# Set a seed to make it deterministic
set.seed(42)

# Calculate the number of training lines (80% of the dataset)
n_training_lines <- floor(0.8 * nrow(college_cleared))

# Ensure the number of training lines is even
if (n_training_lines %% 2 != 0) {
  n_training_lines <- n_training_lines - 1
}


# save index of lines with Private 1 and lines with Private 0
index_dataset_public <- which(college_cleared$Private == 0)

index_dataset_private <- which(college_cleared$Private == 1)

# Calculate the exact percentage of private 1 lines in the dataset
percentage_private <- length(index_dataset_private) / nrow(college_cleared)

# take 80% of lines from index_dataset_public
train_index_dataset_public <- sample(index_dataset_public, floor(n_training_lines * (1 - percentage_private)))
train_index_dataset_private <- sample(index_dataset_private, floor(n_training_lines * (percentage_private)))

# Cobine the two train_index_dataset_* into train_index
train_index <- c(train_index_dataset_public, train_index_dataset_private)

train_data <- college_cleared[train_index,]
test_data <- college_cleared[-train_index,]
```

Now we can fit the glm on the training set:

```{r}
glm <- glm(College$Apps[train_index] ~ .,data=train_data, family=poisson)

#glm_all_parameters <- glm(College$Apps ~ .,data=college_cleared, family=poisson)

#summary(glm_all_parameters)
summary(glm)
```

Now we can calculate the RMSE for the glm:

```{r}
RMSE <- function(predicted, actual) {
  sqrt(mean((predicted - actual)^2))
}


rmse_train_set <- RMSE(predict.glm(glm), College$Apps[train_index])
rmse_test_set <- RMSE(predict.glm(glm, newdata = test_data), College$Apps[-train_index])
```

The rmse for the train_set is inside `rmse_train_set`:

```{r}
rmse_train_set
```

The rmse for the test_set is inside `rmse_test_set`:

```{r}
rmse_test_set
```

The RMSE for the test set is lower than the RMSE for the train set, which is a good sign that the model is not overfitting and it is generalizing well.

# Now we can plot the residuals: YES?
```{r}
# Plot the residuals
plot(glm$residuals, main="Residuals", ylab="Residuals")
```

We will try to fit a random forest model on the training set and see if it performs better than the glm:

```{r}
library(randomForest)
library(randomForestExplainer)

random_forest <- randomForest(x=train_data, formula=Apps ~ ., y=College$Apps[train_index], data = train_data,  ntree = 100, nodesize=1, xtest=test_data, ytest=College$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

random_forest
```

Now we can calculate the RMSE for the random forest model:

```{r}

#RMSE train_set 
rf_rmse_train_set <- sqrt(random_forest$mse[length(random_forest$mse)])

#RMSE test_set
rf_rmse_test_set <- sqrt(random_forest$test$mse[length(random_forest$test$mse)])

```

The rmse for the train_set is inside `rf_rmse_train_set`:

```{r}
rf_rmse_train_set
```

The rmse for the test_set is inside `rf_rmse_test_set`:

```{r}
rf_rmse_test_set
```

The RMSE of the random forest model is lower than the RMSE of the glm model, which means that the random forest model is performing better than the glm model.

Now we can plot how the MSE of the model changes with the number of trees:

```{r}
plot(random_forest, main = "Learning curve of the forest")
```

Now explore how the variables are used in the random forest:

```{r}
measure_importance(random_forest)

plot_multi_way_importance(measure_importance(random_forest), size_measure = "no_of_nodes")
```

And if we increse the number of trees from 100 to 1000:

```{r}

random_forest_1000 <- randomForest(x=train_data, formula=Apps ~ ., y=College$Apps[train_index], data = train_data,  ntree = 1000, nodesize=1, xtest=test_data, ytest=College$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

random_forest_1000

plot(random_forest_1000, main = "Learning curve of the forest")


#RMSE train_set 
rf_rmse_train_set_1000 <- sqrt(random_forest_1000$mse[length(random_forest_1000$mse)])

#RMSE test_set
rf_rmse_test_set_1000 <- sqrt(random_forest_1000$test$mse[length(random_forest_1000$test$mse)])
```

Analyze Root Mean Square Error for the random forest model with 1000 trees:

```{r}
rf_rmse_train_set_1000

rf_rmse_test_set_1000
```

And now how the variables are used:

```{r}
measure_importance(random_forest_1000)
plot_multi_way_importance(measure_importance(random_forest_1000), size_measure = "no_of_nodes")
```

From the graph we can see that there are a number of threes that have better performance:

```{r}

# Extract the index of the minimum from random_forest_1000$mse array:
min_mse_index <- which(random_forest_1000$mse == min_mse)

```

##### Lets go back to the results for the glm model

```{r}
summary(glm)
```


##### There are a couple of takeaways here. First of all the the residual deviance is much lower than the null deviance, so the glm model explains a lot of variance. However the residual deviance od 221,113 is still quite large. Compared to the degrees of freedom this could be due to potential model misspecification, which could be due to non-linearity.


##### Now when we look at back at the multi way importance plot we see that the F.Undergrad and P.Undergrad are the most important features. Lets see the partial dependence plot for these two features.

```{r}
library(pdp)

names <- colnames(train_data)

names <- names[-1]

names

# Partial dependence plot for F.Undergrad
for (pred in names) {
  pdp_obj <- partial(random_forest_1000, pred.var = pred, train = train_data)
  plot(pdp_obj, main = paste("Partial Dependence for", pred))
}

```

##### Most of the features show us that they have a non-linear relatiohship with the response variable.

##### These two points regarding the GLM and the Random forest, suggest that a GAM could be a very strong alternative due to non-linearity between the featuers and the response variable.

##### Now lets fit a GAM on a training set

```{r}
library(mgcv)
# Fit GAM model
gam_model <- gam(College$Apps[train_index]  ~ s(F.Undergrad, k=3) + s(P.Undergrad, k=3) + 
                               Grad.Rate + Outstate + 
                               Room.Board + s(Expend, k=3) + 
                               s(Books, k=3) + s(Personal, k=3) + 
                               s(S.F.Ratio, k=3) + s(perc.alumni, k=3) + 
                               s(Top25perc, k=3) + s(Terminal, k=3),
                 family = poisson, data = train_data, select= TRUE)

summary(gam_model)
```

##### And compare it to the glm

```{r}
AIC(glm, gam_model) 
```

##### I we can see an improvement with the GAM model, a lower AIC and higher degrees of freedom. Lets compare the RMSE to the random forest model.

```{r}
gam_preds_test <- predict(gam_model, test_data, type = "response")

rmse_gam_test <- sqrt(mean((College$Apps[-train_index] - gam_preds_test)^2))

rmse_gam_test


gam_preds_train <- predict(gam_model, train_data, type = "response")

rmse_gam_train <- sqrt(mean((College$Apps[train_index] - gam_preds_train)^2))

rmse_gam_train
```

##### We can also see a slight improvement in RMSE compared to the random forest models. 


## Plot for visualising smooth terms
```{r}

plot(gam_model, pages = 1, shade = TRUE, seWithMean = TRUE)
```


## Comparing predictions of the GAM vs Random forest

```{r}

plot(College$Apps[-train_index], gam_preds_test, col = "blue", pch = 16,
     main = "GAM vs. Random Forest Predictions",
     xlab = "Actual Applications", ylab = "Predicted Applications")
points(College$Apps[-train_index], predict(random_forest_1000, test_data), col = "red", pch = 16)

# Add a perfect prediction reference line
abline(0, 1, col = "black", lwd = 2, lty = 2)

legend("topleft", legend = c("GAM", "Random Forest", "Perfect Prediction"), 
       col = c("blue", "red", "black"), pch = c(16, 16, NA), lty = c(NA, NA, 2), lwd = c(NA, NA, 2))
```
