---
title: "Project: College Dataset"
author: "Stefano Cattonar, Ines El Gataa, Andrija Nicić, Angelica Rota"
date: "2025-02-10"
output:
  pdf_document:
    toc: true
    toc_depth: 3
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading libraries, message=FALSE, warning=FALSE}
library(dplyr)
library(corrplot)
library(caret)
```

# Introduction

This project aims to analyze the `College` dataset from *An Introduction to Statistical Learning* to explore the factors that influence the number of applications received by colleges, represented by the response variable `Apps`. The dataset provides comprehensive information on various attributes of U.S. colleges, such as tuition costs, room and board expenses, financial aid, and student demographics. By examining these factors, we seek to better understand the trends and drivers behind college application rates.

# Preprocessing

## Data Examination

```{r}
College = read.csv("https://www.statlearning.com/s/College.csv", header = TRUE)
help(head)
head(College)
```

`X`: name of the college

`Private`: A factor with levels No and Yes indicating private or public university

`Accept`: Number of applications accepted

`Enroll`: Number of new students enrolled

`Top10perc`: Percentage new students from top 10% of high school class

`Top25perc`: Percentage new students from top 25% of high school class

`F.Undergrad`: Number of full time undergraduates

`P.Undergrad`: Number of part time undergraduates

`Outstate`: Out-of-state tuition

`Room.Board`: Room and board costs

`Books`: Estimated book costs

`Personal`: Estimated personal spending

`PhD`: Percentage of faculty with Ph.D.’s

`Terminal`: Percentage of faculty with terminal degree

`S.F.Ratio`: Student/faculty ratio

`perc.alumni`: Percentage alumni who donate

`Expend`: Instructional expenditure per student

`Grad.Rate`: Graduation rate

**Response variable**

`Apps`: Number of applications received


## Data Exploration 

```{r}
summary(College)
```


### Observations

1. We can note that the `X` column will yield no usable information.

```{r}
College <- select(College, -X)
```

2. We have to exclude also `Accept` and `Enroll` to prevent data leakage.

```{r}
College <- select(College, -c(Accept, Enroll))

summary(College)
```

3. All the variables does not have negative values.

4. The variables does not have NA, so we don't need to replace the missing values

5. We want to check if we have some duplicates

```{r}
idx <- which(duplicated(College))
idx
```

6. The variable `Private` is a categorical one so we convert its values.

```{r}
College$Private <- as.factor(College$Private)

College$Private <- as.numeric(College$Private) - 1  # "Yes" is 1, "No" is 0

College$Private[11:30]
```

7. We can note that `Private` is not balanced 

```{r}


index_dataset_public <- which(College$Private == 0)

index_dataset_private <- which(College$Private == 1)

private_proportion <- (length(index_dataset_private) / length(index_dataset_public))

table(College$Private)
private_proportion
```
We can see from `private_proportion` that "Yes" is 2.66 times more frequent than "No".

Now we can see the barplot

```{r}
barplot(table(College$Private), main="Private")
```

## Variable visualization


 Now we can visualize the variables and their distributions.

 Boxplot, excluded `Private` that is a categorical variable
```{r}

par(mfrow=c(1,3))
for (i in 1:length(College)) {
  if ( names(College[i]) != "Private"){
    boxplot(College[,i], main=names(College[i]))
  }
}
```

Histogram, excluded `Private` that is a categorical variable

```{r}
par(mfrow=c(1,3))
for (i in 1:length(College)) {
  if ( names(College[i]) != "Private"){
    hist(College[,i], main=names(College[i]))
  }
}
```

We can see that F.Undergrad and P.Undergrad are really skewed so we can take the logarithm.

```{r}
College$F.Undergrad <- log(College$F.Undergrad)
College$P.Undergrad <- log(College$P.Undergrad)

par(mfrow = c(2, 2))

boxplot(College$F.Undergrad, main="Full time undergraduates")
boxplot(College$P.Undergrad, main= "Part time undergraduates")

hist(College$F.Undergrad, main="Full time undergraduates")
hist(College$P.Undergrad, main= "Part time undergraduates")
```


## Correlation matrix

We plot the correlation matrix to examine the relationships between the predictors and `Apps`

```{r}
correlation_matrix <- cor(College)

corrplot(correlation_matrix, method="color", tl.cex = 0.8)
```

As you can see from the corralation matrix, there are some variables that are correlated with each other. We can see that the variables `Terminal` and `PhD` are really directly heavily correlated. This fenomena is caused by the fact that `PhD` containes a sub group of `Terminal`. So we can exclude `PhD`.

`Top10perc` and `Top25perc` are the same so for the same reason we can exclude `Top10perc`.

Now we can visualize some of the correlated variables

```{r}
boxplot(College$F.Undergrad ~ College$Private)
boxplot(College$P.Undergrad ~ College$Private)
boxplot(log(College$Apps) ~ College$Private)
```

## Data Normalization
Now let's normalize the data so that we have all the data with comparable scales.

```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

college_norm <- College
college_norm[ , 2:ncol(College)] <- lapply(College[ , 2:ncol(College)], normalize)

summary(college_norm)
```

# GLM

## Dataset preparation

Now we start to clean the dataset as told before:

```{r}

#Copy the dataset
college_cleared <- college_norm

#Remove the target variable and the variables that are not significant
college_cleared$Accept <- NULL
college_cleared$Enroll <- NULL
college_cleared$X <- NULL
college_cleared$Apps <- NULL
college_cleared$Top10perc <- NULL
college_cleared$PhD <- NULL

```

## Dataset split

Now we must split the dataset in train set and test set.
We will use 80% of the dataset for the training and 20% for the test.

```{r}
# Set a seed to make it deterministic
set.seed(42)

# Calculate the number of training lines (80% of the dataset)
n_training_lines <- floor(0.8 * nrow(college_cleared))

# Ensure the number of training lines is even
if (n_training_lines %% 2 != 0) {
  n_training_lines <- n_training_lines - 1
}


# save index of lines with Private 1 and lines with Private 0
index_dataset_public <- which(college_cleared$Private == 0)

index_dataset_private <- which(college_cleared$Private == 1)

# Calculate the exact percentage of private 1 lines in the dataset
percentage_private <- length(index_dataset_private) / nrow(college_cleared)

# take 80% of lines from index_dataset_public
train_index_dataset_public <- sample(index_dataset_public, floor(n_training_lines * (1 - percentage_private)))
train_index_dataset_private <- sample(index_dataset_private, floor(n_training_lines * (percentage_private)))

# Cobine the two train_index_dataset_* into train_index
train_index <- c(train_index_dataset_public, train_index_dataset_private)

train_data <- college_cleared[train_index,]
test_data <- college_cleared[-train_index,]
```

## Model fitting

Now we can fit the glm on the training set:

```{r}
glm <- glm(College$Apps[train_index] ~ .,data=train_data, family=poisson)

summary(glm)
```

## Model evaluation

Now we can calculate the RMSE for the glm:

```{r}
RMSE <- function(predicted, actual) {
  sqrt(mean((predicted - actual)^2))
}


rmse_train_set <- RMSE(predict.glm(glm), College$Apps[train_index])
rmse_test_set <- RMSE(predict.glm(glm, newdata = test_data), College$Apps[-train_index])
```

The rmse for the train_set is inside `rmse_train_set`:

```{r}
rmse_train_set
```

The rmse for the test_set is inside `rmse_test_set`:

```{r}
rmse_test_set
```

The RMSE for the test set is lower than the RMSE for the train set, which is a good sign that the model is not overfitting and it is generalizing well.

# Random Forest

## One hundred trees model fitting

Now, we will try to fit a random forest model on the training set and see if it performs better than the glm:

```{r}
library(randomForest)
library(randomForestExplainer)

random_forest <- randomForest(x=train_data, formula=Apps ~ ., y=College$Apps[train_index], data = train_data,  ntree = 100, nodesize=1, xtest=test_data, ytest=College$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

random_forest
```

## One hundred trees model evaluation

Now we can calculate the RMSE for the random forest model:

```{r}

#RMSE train_set 
rf_rmse_train_set <- sqrt(random_forest$mse[length(random_forest$mse)])

#RMSE test_set
rf_rmse_test_set <- sqrt(random_forest$test$mse[length(random_forest$test$mse)])

```

The rmse for the train_set is inside `rf_rmse_train_set`:

```{r}
rf_rmse_train_set
```

The rmse for the test_set is inside `rf_rmse_test_set`:

```{r}
rf_rmse_test_set
```

The RMSE of the random forest model is lower than the RMSE of the glm model, which means that the random forest model is performing better than the glm model.

Now we can plot how the MSE of the model changes with the number of trees:

```{r}
plot(random_forest, main = "Random forest MSE on train set vs number of trees")
```

Now explore how the variables are used in the random forest:

```{r}
importance_random_forest <- measure_importance(random_forest)

importance_random_forest

plot_multi_way_importance(importance_random_forest, y_measure = "no_of_nodes", x_measure = "times_a_root", size_measure = "no_of_trees", no_of_labels = 13)

```

## One thousand trees model fitting

And if we increse the number of trees from 100 to 1000:

```{r}

random_forest_1000 <- randomForest(x=train_data, formula=Apps ~ ., y=College$Apps[train_index], data = train_data,  ntree = 1000, nodesize=1, xtest=test_data, ytest=College$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

random_forest_1000

plot(random_forest_1000, main = "Random forest MSE on train set vs number of trees")

#RMSE train_set 
rf_rmse_train_set_1000 <- sqrt(random_forest_1000$mse[length(random_forest_1000$mse)])

#RMSE test_set
rf_rmse_test_set_1000 <- sqrt(random_forest_1000$test$mse[length(random_forest_1000$test$mse)])
```
## One thousand trees model evaluation

Analyze Root Mean Square Error for the random forest model with 1000 trees:

```{r}
rf_rmse_train_set_1000

rf_rmse_test_set_1000
```

And now how the variables are used:

```{r}
importance_random_forest_1000 <- measure_importance(random_forest_1000)
plot_multi_way_importance(importance_random_forest_1000, y_measure = "no_of_nodes", x_measure = "times_a_root", size_measure = "no_of_trees", no_of_labels = 13)
```

## Ten thousand trees model fitting

How said in the original paper, the random forest never overfits, so we can try to increase the number of trees to 10000:

```{r}

random_forest_10000 <- randomForest(x=train_data, formula=Apps ~ ., y=College$Apps[train_index], data = train_data,  ntree = 10000, nodesize=1, xtest=test_data, ytest=College$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

random_forest_10000
```


## Ten thousand trees model evaluation

Now see how it performs:

```{r}
plot(random_forest_10000, main = "Random forest MSE on train set vs number of trees")

#RMSE train_set 
rf_rmse_train_set_10000 <- sqrt(random_forest_10000$mse[length(random_forest_10000$mse)])

#RMSE test_set
rf_rmse_test_set_10000 <- sqrt(random_forest_10000$test$mse[length(random_forest_10000$test$mse)])

rf_rmse_train_set_10000

rf_rmse_test_set_10000

importance_random_forest_10000 <- measure_importance(random_forest_10000)

importance_random_forest_10000

plot_multi_way_importance(importance_random_forest_10000, y_measure = "no_of_nodes", x_measure = "times_a_root", size_measure = "no_of_trees", no_of_labels = 13)

```

We can see that the random forest with 10000 trees perform sligthly better the the previous one.

# GAM

## Motivations to using GAM

##### Lets go back to the results for the glm model

```{r}
summary(glm)
```


##### There are a couple of takeaways here. First of all the the residual deviance is much lower than the null deviance, so the glm model explains a lot of variance. However the residual deviance od 221,113 is still quite large. Compared to the degrees of freedom this could be due to potential model misspecification, which could be due to non-linearity.


##### Now when we look at back at the multi way importance plot we see that the F.Undergrad and P.Undergrad are the most important features. Lets see the partial dependence plot for these two features.

```{r}
library(pdp)

names <- colnames(train_data)

names <- names[-1]

names

# Partial dependence plot for F.Undergrad
for (pred in names) {
  pdp_obj <- partial(random_forest_1000, pred.var = pred, train = train_data)
  plot(pdp_obj, main = paste("Partial Dependence for", pred))
}

```

##### Most of the features show us that they have a non-linear relatiohship with the response variable.

##### These two points regarding the GLM and the Random forest, suggest that a GAM could be a very strong alternative due to non-linearity between the featuers and the response variable.

## Model fitting and fine tuning

##### Now lets fit a GAM on a training set

```{r}
library(mgcv)
# Fit GAM model
gam_model <- gam(College$Apps[train_index]  ~ F.Undergrad + P.Undergrad + 
                               Grad.Rate + Outstate + 
                               Room.Board + Expend + 
                               Books + Personal + 
                               S.F.Ratio + perc.alumni + 
                               Top25perc + Terminal,
                 family = poisson, data = train_data)

summary(gam_model)
```

##### And compare it to the glm

```{r}
gam_preds_test <- predict(gam_model, test_data, type = "response")

rmse_gam_test <- sqrt(mean((College$Apps[-train_index] - gam_preds_test)^2))

rmse_gam_test

gam_preds_train <- predict(gam_model, train_data, type = "response")

rmse_gam_train <- sqrt(mean((College$Apps[train_index] - gam_preds_train)^2))

rmse_gam_train
```

##### We can also see a slight improvement in RMSE compared to the random forest models. 



##### Now lets fine-tune the model a bit to see if we can get even better results

```{r}
library(mgcv)
# Fit GAM model
gam_model_2 <- gam(College$Apps[train_index]  ~ s(F.Undergrad, k=3) + s(P.Undergrad, k=3) + 
                               Grad.Rate + Outstate + 
                               Room.Board + s(Expend, k=3) + 
                               s(Books, k=3) + s(Personal, k=3) + 
                               s(S.F.Ratio, k=3) + s(perc.alumni, k=3) + 
                               s(Top25perc, k=3) + s(Terminal, k=3),
                 family = poisson, data = train_data, select= TRUE)

summary(gam_model_2)
```

##### And lets see the RMSE

```{r}
gam_preds_test_2 <- predict(gam_model_2, test_data, type = "response")

rmse_gam_test_2 <- sqrt(mean((College$Apps[-train_index] - gam_preds_test_2)^2))

rmse_gam_test_2


gam_preds_train_2 <- predict(gam_model_2, train_data, type = "response")

rmse_gam_train_2 <- sqrt(mean((College$Apps[train_index] - gam_preds_train_2)^2))

rmse_gam_train_2
```
## Comparing the models

```{r}
AIC(gam_model,gam_model_2)
```


#### Comparing predictions of the GAM vs Random forest \


```{r}

plt_num <- length(College$Apps[-train_index])-1

plot(0:plt_num, gam_preds_test, col = "blue", pch = 16,
     main = "GAM vs. Random Forest Predictions",
     xlab = "School Index", ylab = "Applications")


points(0:plt_num, predict(random_forest_1000, test_data), col = "red", pch = 16)
points(0:plt_num, College$Apps[-train_index], col = "black", pch = 16)

for (i in 0:plt_num) {
  abline(v = i, col = "gray", lwd = 0.5, lty = 2)  # Dashed thin vertical lines
}


legend("topleft",inset=c(0,-0.1), legend = c("Actual Value", "Random Forest","GAM"), 
       col = c("black", "red", "blue"), pch = c(16, 16, 16), bty = "n", horiz=TRUE,xpd=TRUE)


```
