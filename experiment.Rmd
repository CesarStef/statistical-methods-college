---
title: "Project: College Dataset"
author: "Stefano Cattonar, Ines El Gataa, Andrija NiciÄ‡, Angelica Rota"
date: "2025-01-90"
output:
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

#A random forest regression

# Load the data
dataset <- read.csv("College.csv")

# Split the data into training and testing sets
set.seed(0)

# Drop dataset$Enroll and drop dataset$Accept
dataset_cleared <- dataset
dataset_cleared$Accept <- NULL # <- dataset_cleared$Accept <- dataset_cleared$Enroll <- NULL 
dataset_cleared$Enroll <- NULL

dataset_cleared$X <- NULL

dataset_cleared$Private <- as.factor(dataset_cleared$Private)

dataset_cleared$Private <- as.numeric(dataset_cleared$Private) - 1

########

dataset_cleared$F.Undergrad <- log(dataset_cleared$F.Undergrad)

dataset_cleared$P.Undergrad <- log(dataset_cleared$P.Undergrad)

### WITH LOG WE LOST 0.1% OF R2 ON THE TRAINING SET

normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

dataset_cleared[ , 2:ncol(dataset_cleared)] <- lapply(dataset_cleared[ , 2:ncol(dataset_cleared)], normalize)

#####


# Separate the data into training and testing sets

# Calculate the number of training lines (80% of the dataset)
n_training_lines <- floor(0.8 * nrow(dataset_cleared))

# Ensure the number of training lines is even
if (n_training_lines %% 2 != 0) {
  n_training_lines <- n_training_lines - 1
}


# save index of lines with Private 1 and lines with Private 0
index_dataset_public <- which(dataset_cleared$Private == 0)

index_dataset_private <- which(dataset_cleared$Private == 1)

# Calculate the exact percentage of private 1 lines in the dataset
percentage_private <- length(index_dataset_private) / nrow(dataset_cleared)

# take 80% of lines from index_dataset_public
train_index_dataset_public <- sample(index_dataset_public, floor(n_training_lines * (1 - percentage_private)))
train_index_dataset_private <- sample(index_dataset_private, floor(n_training_lines * (percentage_private)))

# Cobine the two train_index_dataset_ into train_index
train_index <- c(train_index_dataset_public, train_index_dataset_private)


train_data <- dataset_cleared[train_index,]
test_data <- dataset_cleared[-train_index,]


# Separate train_data and test_data into predictors and response

train_data$Apps <- NULL
test_data$Apps <- NULL

# Response

#train_data$Apps <- dataset$Apps[train_index]



# Check the structure of the training and testing sets
#names(train_data)
#names(test_data)

#setdiff(names(train_data), names(test_data))


# Do some bootstrap sampling to get a sense of the variability of the random forest model

# Fit the random forest model

library(randomForest)

rf_model <- randomForest(x=train_data, formula=Apps ~ ., y=dataset$Apps[train_index], data = train_data,  ntree = 77, nodesize=1, xtest=test_data, ytest= dataset$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE)

#rf_model$proximity


rf_model$type

rf_model$importance

#rf_model$forest

mean(rf_model$mse)
# Make predictions

rf_pred <- predict(rf_model, test_data)

# Calculate the RMSE

rmse <- sqrt(mean((rf_pred - dataset$Apps[-train_index])^2))

#rmse

mean(rf_model$rsq)
mean(rf_model$test$rsq)

# Plot the results, blu dots are the real values, red dots are the predicted values

plot(dataset$Apps[-train_index], col = "blue", pch = 20, xlab = "Index", ylab = "Apps")
points(rf_pred, col = "red", pch = 20)


# Plot the MSE of the model
pippo <- plot(rf_model, type="l", log="y")

```


```{r}
library(randomForestExplainer)

rf_explained_model <- randomForest(x=train_data, formula=Apps ~ ., y=dataset$Apps[train_index], data = train_data,  ntree = 100, nodesize=1, xtest=test_data, ytest= dataset$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

plot(rf_explained_model, main = "Learning curve of the forest")
legend("topright", c("error for 'dead'", "misclassification error", "error for 'alive'"), lty = c(1,1,1), col = c("green", "black", "red"))

rf_explained_model

min_depth_frame <- min_depth_distribution(rf_explained_model)
min_depth_frame
plot_min_depth_distribution(min_depth_frame, k=16)

importance_frame <- measure_importance(rf_explained_model)
importance_frame


plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")

train_data$Top10perc <- NULL
train_data$PhD <- NULL
train_data$Personal <- NULL
train_data$Books <- NULL

test_data$Top10perc <- NULL
test_data$PhD <- NULL
test_data$Personal <- NULL
test_data$Books <- NULL

i = length(test_data)
# TODO: mtry now is the number of predictors, we should check the paper about Extremely Randomized Trees, I'm not sure if this is correct...
rf_new <- randomForest(x=train_data, formula=Apps ~ ., y=dataset$Apps[train_index], data = train_data,  ntree = 100, nodesize=1, xtest=test_data, ytest= dataset$Apps[-train_index], keep.forest = TRUE, replace = TRUE, proximity=TRUE, oob.prox = FALSE, importance = TRUE, localImp = TRUE)

rf_new

plot_multi_way_importance(measure_importance(rf_new), size_measure = "no_of_nodes")

plot(rf_new, main = "Learning curve of the forest")

measure_importance(rf_new)
#help("explain_forest")
#explain_forest(rf_explained_model, interactions = TRUE, data = train_data)
```